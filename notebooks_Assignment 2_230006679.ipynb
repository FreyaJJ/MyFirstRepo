{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a35eeb9f-df70-4ab1-a243-2d2025888eb0",
      "cell_type": "markdown",
      "source": "# Python for Spatial Analysis\n## Lab Assignment 2 - GG3209 - Python Part\n### GitHub Repository: https://github.com/FreyaJJ/MyFirstRepo.git\n\n---\nStudent ID: 230006679 - University of St Andrews - School of Geography and Sustainable Development",
      "metadata": {}
    },
    {
      "id": "77bce54f-a467-47ee-abcb-a0dbc4cf988c",
      "cell_type": "markdown",
      "source": "### Introduction\n\nThis Jupyter notebook is a collection of the four parts to assessment 2 which uses ArcGIS Online and Google Colab to code the following.\n\n### Content:\n\n#### 1. Python Basics:\n\n* Exercise 1: Create a script that calculates the average of a list of numbers\n* Exercise 2: Whats the purpose\n* Exercise 3: Indentation\n* Exercise 4: Strings\n* Exercise 5: Nested *if* conditions\n* Exercise 6: Functions\n* Exercise 7: Create a script that generates a multiplicaation table\n* Exercise 8: Loops\n* Exercise 9: Read files\n\n#### 2. NumPy and Pandas:\n\nNumPy\n* Exercise 1: Creating arrays\n* Exercise 2: Create and reshape an array\n* Exercise 3: Create a linearly spaced array\n* Exercise 4: Create and index an array\n* Exercise 5: Calculate the sum of all numbers\n* Exercise 6: Calculate the sum of each row\n* Exercise 7: Boolean mask: extract values greater than the mean\n\nPandas\n* Exercise 1: Importing Pandas\n* Exercise 2: Import a CSV dataset\n* Exercise 3: Rows and columns in the dataframe\n* Exercise 4: Mean of category in the whole dataset\n* Exercise 5: Max of category in the whole dataset\n* Exercise 6: Calculate countries produce more emissions than 1000 Kg/CO2\n* Exercise 7: Calculate country consumes the least amount of beef\n* Exercise 8: Calculate total emissions of meat products in dataset\n* Exercise 9: Calculate total emissions of all other products in dataset\n\nFinal Exercise\n1. Import pandas and read CSV dataset\n2. Calculate new column\n3. Remove column\n4. Subset a city starting with the letter F\n5. Subset the five biggest cities from the country where the F city is \n  \n#### 3. GeoPandas and Rasterrio\n\nGeoPandas\n* Exercise 1: Load the Shapefile into a GeoDataFrame\n* Exercise 2: Subset relevant columns for analysis\n* Exercise 3: Inspect the coordinate reference system (CRS)\n* Exercise 4: Check the nmber of feature in the layer\n* Exercise 5: Ensure uniqueness of the LSOA Area Code (LSOA11CD)\n* Exercise 6: Visualize the Layer Using .plot()\n* Exercise 7: Explore the Layer with .explorer()\n* Exercise 8: Subset Areas with Population Greater Than 1500\n* Exercise 9: Visualize the Subset with Population-Based Symbology\n* Exercise 10: Count the Number of Areas in the Subset\n* Exercise 11: Calculate the Total Population of the Subset\n\nRasterrio\n* Exercise 1: Read the Raster File Using Rasterio\n* Exercise 2: Inspect the Coordinate Reference System (CRS) of the Raster\n* Exercise 3: Check the Rasters Extent (Bounds) in Projected Coordinates\n* Exercise 4: Determine the Number of Bands in the Raster Dataset\n* Exercise 5: Visualize the Raster Image\n* Exercise 6: Generate Histograms from the Raster Data\n* Exercise 7: Create a False-Color Plot Using EarthPy (with Troubleshooting Tips)\n\n#### 4. Spatial Clustering (K-Means - DBSCAN)\n\nPart 1: Get and read the large dataset\n1. Download the Road Accident (United Kingdom (UK)) dataset\n2. Upload the dataset in your Google Drive\n3. Mount the Drive to Notebook\n\n\nPart 2: Exploratory Data Analysis and K-means Clustering\n\n(A) - Data Exploration and Pre-Processing\n* Exercise 1: Load the Dataset Using Pandas\n* Exercise 2: Preview the Data Structure and Attributes\n* Exercise 3: Select Relevant Numerical and Categorical Columns\n* Exercise 4: Filter the Dataset to Include Only 2010 Records\n* Exercise 5: Visualize Accidents by Day of the Week\n* Exercise 6: Analyze Severity vs. Road Conditions with a Second Plot\n* Exercise 7: Map All Accidents Using the Lonboard Library\n* Exercise 8: Apply Spatial Filtering and Map the Glasgow-Edinburgh Region\n\n(B) - K-means Clustering Implementation\n* Exercise 1: Apply K-Means Clustering with Multiple k Values\n* Exercise 2: Mapping Cluster Results Using the Lonboard Library\n* Exercise 3: Interpretation: Effect of k on Cluster Structure\n* Exercise 4: K-Means with Additional Attributes (Severity, Number of Vehicles, etc.)\n* Exercise 5: Mapping Multivariate Clusters with Lonboard\n* Exercise 6: Reflection: Coordinate-Only vs. Attribute-Enhanced Clustering\n\n\nPart 3: Spatial Analysis and DBSCAN Clustering\n\n(A) - Spatial Correlation\n* Exercise 1: Create a New GeoDataFrame for DBSCAN Analysis\n* Exercise 2: Filter the GeoDataFrame to Include Only Birmingham Accidents (Using BBox)\n* Exercise 3: Map Birmingham Accident Data Using Lonboard\n* Exercise 4: Inspect Attribute Data Types (Numerical vs. Categorical)\n* Exercise 5: Compute Correlation Matrix for Numerical Features\n* Exercise 6: Visualize the Correlation Matrix with a Heatmap\n* Exercise 7: Install Required Library: pysal\n* Exercise 8: Import Additional Required Libraries\n* Exercise 9: Reproject the Dataset to the Appropriate UK Coordinate Reference System (EPSG)\n* Exercise 10: Interpretation: Insights from the Correlation Analysis\n\n(B) - DBSCAN Clustering Implementation\n* Exercise 1: Apply DBSCAN Clustering with Varying eps and min_samples Values\n* Exercise 2: Map DBSCAN Cluster Results Using Plotly\n* Exercise 3: Interpretation: Effect of Parameter Changes on DBSCAN Clustering\n* Exercise 4: Reflection: Comparing DBSCAN to K-Means Clustering\n* Exercise 5: Discussion: Real-World Urban Planning Implications of Identified Clusters\n\n---",
      "metadata": {}
    },
    {
      "id": "cea27b48-f2c1-412b-bc05-3f101542e516",
      "cell_type": "markdown",
      "source": "## Part 1: Python Basics",
      "metadata": {}
    },
    {
      "id": "ec1519ae-3d59-43b2-8eee-7ec3a335c8d5",
      "cell_type": "markdown",
      "source": "### Exercise 1: Create a script that calculates the average of a list of numbers\n#### Steps:\n* In a new cell I created a list of numbers: [1,2,3,4,5,6,7,8,9,10]\n* The I wrote a function that takes a list of numbers as input, calculates the average, and returns the result.\n* Finally I called the function with my list of numbers and printed the result.",
      "metadata": {}
    },
    {
      "id": "07c6af32-b8d7-4b6b-8b9a-1fdf04f0d7d4",
      "cell_type": "code",
      "source": "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\naverage = sum(numbers) / len(numbers)\nprint(average)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "5.5\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "f0e04220-31af-4e47-a6a8-9af97f6772d6",
      "cell_type": "markdown",
      "source": "\nThe result was an average of 5.5, as all the numbers were added together (55) and divided by the lenth of the numbers list (10).",
      "metadata": {}
    },
    {
      "id": "08d5a5e3-19bf-4360-873c-ce3602ff6c88",
      "cell_type": "markdown",
      "source": "### Exercise 2: What's the purpose\n#### Steps:\n* We were asked what the pupose of the first two expressions and explain what they do, with the following code below:\n  \n  ```\n  import pandas as pd\n  import geopandas as gpd\n  dat = pd.read_csv('data/world_cities.csv')  ## Import CSV file\n  geom = gpd.points_from_xy(dat['lon'], dat['lat'])\n  geom = gpd.GeoSeries(geom)\n  dat = gpd.GeoDataFrame(dat, geometry = gpd.GeoSeries(geom), crs = 4326)\n  dat.to_file('output/world_cities.shp')      ## Export Shapefile\n  ```",
      "metadata": {}
    },
    {
      "id": "534e57e2-2afa-46fc-b45f-7a0856f0fd1d",
      "cell_type": "markdown",
      "source": "The first two expressions imports the pandas (abbreviated to pd) and geopandas (abbreviated to qpd) libraries into Python. This then allows us convert a CSV file to a Shapefile through reading the CSV file, creating geometry from longitiude and latitude, converting the geometry to a GeoSeries, before creatng a GeoDataFrame and finally exporting to a shapefile. ",
      "metadata": {}
    },
    {
      "id": "cda84731-5751-4b7a-9180-bdf4f63fa525",
      "cell_type": "markdown",
      "source": "### Exercise 3: Indentation\n#### Steps:\n* We were given the following code cell and asked to address the issue when running it:\n\n```\nname = 'Dave'\n    dogs = 0\nprint('My name is', name, 'and I own', dogs, 'dogs.')\n```",
      "metadata": {}
    },
    {
      "id": "5c4b74ea-4d58-4cbb-8410-644fb63ecdcc",
      "cell_type": "code",
      "source": "name = 'Dave'\ndogs = 0\nprint('My name is', name, 'and I own', dogs, 'dogs.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "My name is Dave and I own 0 dogs.\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "bfc810ab-6063-4d50-81ee-17b03035eb31",
      "cell_type": "markdown",
      "source": "### Exercise 4: Strings\n#### Steps:\n* Created two string variables\n* Joined the two strings together\n* Printed the strings and integer together",
      "metadata": {}
    },
    {
      "id": "29bf0316-76e8-492b-83bf-59bf5234e83a",
      "cell_type": "code",
      "source": "a = \"Thank \"\nb = \"you\"\nc = a + b\nprint(c)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Thank you\n"
        }
      ],
      "execution_count": 6
    },
    {
      "id": "f723668e-b07a-4549-ba6d-6578a5217a93",
      "cell_type": "markdown",
      "source": "### Exercise 5: Nested *if* conditions\n#### Steps:\n* Created the variables where genre stores the movie genre as a string and duration stores the length of the movie in hours.\n* Created outer 'if' statements for movie genres\n* Created nested 'if' and 'else' statements are excuted if the answer is True or False depending on the duration of movie.\n* Created an extending if-else statement for more genres if previous statements are false\n* Created another extending if-else statement if all of the above is false",
      "metadata": {}
    },
    {
      "id": "ddf7eeec-bbf2-41d1-8757-0e25d17c0f31",
      "cell_type": "code",
      "source": "genre = \"Action\" \nduration = 2.5\n\nif genre == \"Action\":\n\n     if duration > 3:\n         print(\"Buy lots of popcorn! This will be a long action packed movie!\")\n\n     else:\n         print(\"Short action movie! Great for a quick escape during a busy day!\")\n\nelif genre == \"Romcom\":\n\n     if duration > 2:\n         print(\"A long romcom! Get ready for love triangles, meet-cutes and complex relationships!\")\n\n     else:\n         print(\"Short and sweet! Perfect for a heartwarming midday pick-me-up!\")\nelse:\n     print(\"Hmm! I'm not sure about this genre! Maybe check the reviews before watching!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Short action movie! Great for a quick escape during a busy day!\n"
        }
      ],
      "execution_count": 7
    },
    {
      "id": "f2166912-d6c8-4f82-b350-1fcfc0303f84",
      "cell_type": "markdown",
      "source": "### Exercise 6: Functions\n#### Steps:\n* Defined the function\n* Made an indentation and converted miles to kilometers (1m = 1.60934km) before returning the results\n* Called the function and printed the result (using an example of 8 miles being coverted to kilometers)",
      "metadata": {}
    },
    {
      "id": "fb0b3e8f-9e2b-4c91-82d2-2397b77ae958",
      "cell_type": "code",
      "source": "def miles_to_kms(miles):\n    return miles * 1.60934\n\nprint(miles_to_kms(8))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "12.87472\n"
        }
      ],
      "execution_count": 8
    },
    {
      "id": "740dea39-ded2-4d87-9728-76546455f02e",
      "cell_type": "markdown",
      "source": "### Exercise 7: Create a script that generates a multiplication table\n#### Steps:\n* Wrote a fuction that takes an integer as input and generates a multiplication table for that number, from 1 to 10 (using a loop)\n* Called the function with a few different numbers and printed the results.",
      "metadata": {}
    },
    {
      "id": "a28f3ada-2763-438b-b78c-9ca071d60651",
      "cell_type": "code",
      "source": "def multiplication_table(number):\n    for i in range(1, 11):\n        print(f\"{number} x {i} = {number * i}\")\n\nnumbers = [2, 4, 6]\nfor num in numbers:\n    print(f\"\\nMultiplication table for {num}:\")\n    multiplication_table(num)\n        ",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\nMultiplication table for 2:\n2 x 1 = 2\n2 x 2 = 4\n2 x 3 = 6\n2 x 4 = 8\n2 x 5 = 10\n2 x 6 = 12\n2 x 7 = 14\n2 x 8 = 16\n2 x 9 = 18\n2 x 10 = 20\n\nMultiplication table for 4:\n4 x 1 = 4\n4 x 2 = 8\n4 x 3 = 12\n4 x 4 = 16\n4 x 5 = 20\n4 x 6 = 24\n4 x 7 = 28\n4 x 8 = 32\n4 x 9 = 36\n4 x 10 = 40\n\nMultiplication table for 6:\n6 x 1 = 6\n6 x 2 = 12\n6 x 3 = 18\n6 x 4 = 24\n6 x 5 = 30\n6 x 6 = 36\n6 x 7 = 42\n6 x 8 = 48\n6 x 9 = 54\n6 x 10 = 60\n"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "b878940a-21a2-4504-8eea-13b9ff1fc33e",
      "cell_type": "markdown",
      "source": "### Exercise 8: Loops\n#### Steps:\n* Defined a list named lunch, as shown below\n* Created an empty dictionary for the list\n* Used a *for* loop to go over the items, and a conditional to add a new item with count 1 (if it is not yet in the dictionary), or to increment an existing item count.\n* Printed results",
      "metadata": {}
    },
    {
      "id": "348261a3-2b31-4fdd-afa9-d64091df8a0a",
      "cell_type": "code",
      "source": "lunch = ['Salad', 'Salad', 'Egg', 'Beef', 'Potato', 'Tea', 'chicken', 'Potato', 'Potato', 'Coffee']\n\nlunch_count = {}\n\nfor item in lunch:\n    if item in lunch_count:\n        lunch_count[item] += 1\n    else:\n        lunch_count[item] = 1\n\nprint(lunch_count)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'Salad': 2, 'Egg': 1, 'Beef': 1, 'Potato': 3, 'Tea': 1, 'chicken': 1, 'Coffee': 1}\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "196fa21d-3976-4222-af12-b893bb435db8",
      "cell_type": "markdown",
      "source": "### Exercise 9: Read files\n#### Steps:\n* Created a script that reads and prints the latest_earthquake_world.csv file, included in the data folder.",
      "metadata": {}
    },
    {
      "id": "cf857cf7-2f8e-461b-b7f6-cbfd0753707a",
      "cell_type": "code",
      "source": "dataset = 'Latest_earthquake_world.csv'\n\nimport csv\n\nwith open('Latest_earthquake_world.csv', \"r\") as csvfile:\n    reader = csv.reader(csvfile)\n    \nimport pandas as pd\n\nearthquake_df = pd.read_csv('Latest_earthquake_world.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\nearthquake_df.head(30)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                        time   latitude   longitude    depth   mag magType  \\\n0   2023-02-23T08:01:02.881Z  38.128000   73.218400   10.000  4.70      mb   \n1   2023-02-23T06:55:34.020Z  18.794600  -63.920500   10.000  3.67      md   \n2   2023-02-23T06:50:49.137Z  38.487900   72.812200   10.000  4.50      mb   \n3   2023-02-23T06:18:13.280Z  -8.548300  -77.625400   66.540  4.70      mb   \n4   2023-02-23T03:36:09.429Z -18.287000 -177.826100  524.983  4.90      mb   \n5   2023-02-23T03:18:57.663Z -30.391600  -71.685600   34.097  4.40      ml   \n6   2023-02-23T02:18:00.671Z  38.336200   73.106700   10.000  4.70      mb   \n7   2023-02-23T02:07:46.940Z  38.182200   73.234600   10.000  4.80      mb   \n8   2023-02-23T02:03:42.690Z  18.079300  -68.088800   78.000  3.54      md   \n9   2023-02-23T01:35:57.658Z  38.182400   73.279400   10.000  4.90      mb   \n10  2023-02-23T01:33:25.965Z  59.261500 -153.135600   77.400  2.50      ml   \n11  2023-02-23T01:06:05.337Z  38.180400   72.782900   10.000  4.60      mb   \n12  2023-02-23T00:55:14.540Z  38.147400   72.982400   10.000  5.00      mb   \n13  2023-02-23T00:37:40.397Z  38.072600   73.207700   20.522  6.80     mww   \n14  2023-02-22T23:44:02.170Z  39.145168 -123.260002    1.870  2.91      md   \n15  2023-02-22T23:14:55.147Z  64.931800 -147.252000  148.800  2.90      ml   \n16  2023-02-22T23:06:22.156Z  53.653800   98.458200    9.707  4.70      mb   \n17  2023-02-22T22:05:53.566Z  10.179100  125.063300   10.000  4.60      mb   \n18  2023-02-22T21:39:43.663Z  59.184300 -151.517400    2.000  2.50      ml   \n19  2023-02-22T21:18:44.007Z  23.963800  121.774100    8.774  4.40      mb   \n20  2023-02-22T20:01:57.120Z  38.820999 -122.853996    2.640  3.22      ml   \n21  2023-02-22T18:22:12.682Z  38.157700 -117.889100    8.700  3.50      ml   \n22  2023-02-22T18:18:25.802Z -35.700800  -73.161700   35.000  4.30      mb   \n23  2023-02-22T17:47:55.177Z -35.748700  -73.151800   35.000  4.00      mb   \n24  2023-02-22T15:56:47.583Z  61.257000 -150.320300  250.000  2.50      ml   \n25  2023-02-22T15:51:39.859Z -30.038900  -72.203500   12.059  4.00     mwr   \n26  2023-02-22T15:48:21.690Z  40.555832 -124.212669   14.430  3.21      ml   \n27  2023-02-22T15:18:06.170Z  18.231000  -68.260500   96.000  3.76      md   \n28  2023-02-22T14:11:28.720Z  19.904167 -155.158333   43.590  3.24      ml   \n29  2023-02-22T13:12:26.230Z  18.798667  -65.057500   17.130  3.09      md   \n\n      nst     gap      dmin     rms  ...                   updated  \\\n0    75.0   55.00  1.745000  0.6600  ...  2023-02-23T08:35:43.040Z   \n1    19.0  228.00  1.126400  0.5000  ...  2023-02-23T08:02:57.172Z   \n2    21.0  171.00  1.264000  0.5600  ...  2023-02-23T07:28:35.040Z   \n3    44.0  154.00  3.503000  0.4700  ...  2023-02-23T07:15:40.040Z   \n4    40.0   63.00  3.626000  0.6700  ...  2023-02-23T03:50:54.040Z   \n5    33.0  187.00  0.285000  0.9400  ...  2023-02-23T03:30:52.786Z   \n6    52.0   60.00  1.529000  0.9400  ...  2023-02-23T02:37:56.040Z   \n7    21.0  180.00  1.712000  0.8100  ...  2023-02-23T02:22:17.040Z   \n8    14.0  234.00  0.157700  0.2000  ...  2023-02-23T02:42:39.960Z   \n9    27.0  166.00  1.735000  0.7000  ...  2023-02-23T01:49:27.040Z   \n10    NaN     NaN       NaN  0.5700  ...  2023-02-23T01:35:49.711Z   \n11   20.0  163.00  1.506000  0.6400  ...  2023-02-23T01:27:51.040Z   \n12   84.0   80.00  1.618000  0.8400  ...  2023-02-23T01:13:06.040Z   \n13  128.0   20.00  1.783000  0.9600  ...  2023-02-23T05:49:51.952Z   \n14   26.0   68.00  0.068420  0.0800  ...  2023-02-23T06:29:31.972Z   \n15    NaN     NaN       NaN  1.2900  ...  2023-02-22T23:18:24.121Z   \n16   64.0   68.00  3.724000  0.6700  ...  2023-02-23T00:02:22.040Z   \n17   44.0  120.00  3.131000  0.4600  ...  2023-02-23T04:25:16.518Z   \n18    NaN     NaN       NaN  1.0800  ...  2023-02-22T21:42:36.481Z   \n19   34.0   83.00  0.266000  0.5100  ...  2023-02-22T23:06:06.040Z   \n20   52.0   49.00  0.006912  0.0500  ...  2023-02-23T03:23:48.835Z   \n21   13.0   80.53  0.059000  0.1197  ...  2023-02-23T02:03:07.570Z   \n22   39.0  150.00  1.045000  0.7200  ...  2023-02-22T21:45:30.040Z   \n23   26.0  198.00  0.998000  0.6600  ...  2023-02-22T21:40:26.040Z   \n24    NaN     NaN       NaN  0.9300  ...  2023-02-22T21:02:30.323Z   \n25   23.0  179.00  0.801000  1.1600  ...  2023-02-22T17:15:55.040Z   \n26   21.0  177.00  0.234100  0.1200  ...  2023-02-23T00:44:05.211Z   \n27   15.0  205.00  0.307600  0.2700  ...  2023-02-22T15:54:41.369Z   \n28   45.0  256.00       NaN  0.1200  ...  2023-02-23T08:07:59.549Z   \n29    8.0  259.00  0.454100  0.2100  ...  2023-02-22T13:35:31.700Z   \n\n                                            place        type horizontalError  \\\n0                  65 km W of Murghob, Tajikistan  earthquake            4.00   \n1                                 Leeward Islands  earthquake            5.27   \n2               106 km WNW of Murghob, Tajikistan  earthquake            8.00   \n3                       22 km SW of Quiches, Peru  earthquake            8.10   \n4                                             NaN  earthquake           13.43   \n5                      52 km WNW of Ovalle, Chile  earthquake            5.75   \n6               Tajikistan-Xinjiang border region  earthquake            6.56   \n7                  64 km W of Murghob, Tajikistan  earthquake            6.80   \n8   64 km ESE of Boca de Yuma, Dominican Republic  earthquake            1.66   \n9                  60 km W of Murghob, Tajikistan  earthquake            7.86   \n10                    69 km W of Nanwalek, Alaska  earthquake             NaN   \n11                103 km W of Murghob, Tajikistan  earthquake            7.16   \n12                                     Tajikistan  earthquake            4.66   \n13                 67 km W of Murghob, Tajikistan  earthquake            6.93   \n14                             4km W of Ukiah, CA  earthquake            0.21   \n15                12 km WNW of Two Rivers, Alaska  earthquake             NaN   \n16                     122 km WSW of Ikey, Russia  earthquake            7.42   \n17                6 km WNW of Liloan, Philippines  earthquake           10.85   \n18                25 km SE of Port Graham, Alaska  earthquake             NaN   \n19                17 km E of Hualien City, Taiwan  earthquake            2.29   \n20                    10km WNW of The Geysers, CA  earthquake            0.15   \n21                       32 km SE of Mina, Nevada  earthquake             NaN   \n22                                            NaN  earthquake            4.88   \n23                  78 km WNW of Cauquenes, Chile  earthquake            4.98   \n24                                            NaN  earthquake             NaN   \n25                     83 km W of Coquimbo, Chile  earthquake            2.83   \n26                        5km ESE of Ferndale, CA  earthquake            0.78   \n27                                   Mona Passage  earthquake            1.72   \n28                      5 km NW of Honomu, Hawaii  earthquake            0.59   \n29                             Puerto Rico region  earthquake            1.15   \n\n   depthError  magError  magNst     status  locationSource magSource  \n0       1.907  0.058000    91.0   reviewed              us        us  \n1       4.420  0.130000    14.0   reviewed              pr        pr  \n2       2.000  0.140000    15.0   reviewed              us        us  \n3       8.700  0.064000    74.0   reviewed              us        us  \n4      10.963  0.108000    27.0   reviewed              us        us  \n5       5.597       NaN     NaN   reviewed              us       guc  \n6       1.897  0.059000    87.0   reviewed              us        us  \n7       1.992  0.226000     6.0   reviewed              us        us  \n8       1.340  0.060000     6.0   reviewed              pr        pr  \n9       1.985  0.177000    10.0   reviewed              us        us  \n10      0.700       NaN     NaN  automatic              ak        ak  \n11      1.772  0.207000     7.0   reviewed              us        us  \n12      1.768  0.071000    63.0   reviewed              us        us  \n13      4.439  0.065000    23.0   reviewed              us        us  \n14      8.330  0.180000    32.0  automatic              nc        nc  \n15     14.300       NaN     NaN  automatic              ak        ak  \n16      4.501  0.053000   106.0   reviewed              us        us  \n17      1.887  0.077000    51.0   reviewed              us        us  \n18      3.800       NaN     NaN  automatic              ak        ak  \n19      4.572  0.126000    18.0   reviewed              us        us  \n20      0.240  0.445000    10.0  automatic              nc        nc  \n21      1.200  0.210000     9.0   reviewed              nn        nn  \n22      1.955  0.238000     7.0   reviewed              us        us  \n23      2.007  0.519000     1.0   reviewed              us        us  \n24      5.100       NaN     NaN  automatic              ak        ak  \n25      6.844  0.050000    38.0   reviewed              us        us  \n26      0.740  0.302000    10.0  automatic              nc        nc  \n27      1.080  0.110000    11.0   reviewed              pr        pr  \n28      0.950  0.127770    25.0   reviewed              hv        hv  \n29      1.640  0.037425     4.0   reviewed              pr        pr  \n\n[30 rows x 22 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>depth</th>\n      <th>mag</th>\n      <th>magType</th>\n      <th>nst</th>\n      <th>gap</th>\n      <th>dmin</th>\n      <th>rms</th>\n      <th>...</th>\n      <th>updated</th>\n      <th>place</th>\n      <th>type</th>\n      <th>horizontalError</th>\n      <th>depthError</th>\n      <th>magError</th>\n      <th>magNst</th>\n      <th>status</th>\n      <th>locationSource</th>\n      <th>magSource</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-02-23T08:01:02.881Z</td>\n      <td>38.128000</td>\n      <td>73.218400</td>\n      <td>10.000</td>\n      <td>4.70</td>\n      <td>mb</td>\n      <td>75.0</td>\n      <td>55.00</td>\n      <td>1.745000</td>\n      <td>0.6600</td>\n      <td>...</td>\n      <td>2023-02-23T08:35:43.040Z</td>\n      <td>65 km W of Murghob, Tajikistan</td>\n      <td>earthquake</td>\n      <td>4.00</td>\n      <td>1.907</td>\n      <td>0.058000</td>\n      <td>91.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-02-23T06:55:34.020Z</td>\n      <td>18.794600</td>\n      <td>-63.920500</td>\n      <td>10.000</td>\n      <td>3.67</td>\n      <td>md</td>\n      <td>19.0</td>\n      <td>228.00</td>\n      <td>1.126400</td>\n      <td>0.5000</td>\n      <td>...</td>\n      <td>2023-02-23T08:02:57.172Z</td>\n      <td>Leeward Islands</td>\n      <td>earthquake</td>\n      <td>5.27</td>\n      <td>4.420</td>\n      <td>0.130000</td>\n      <td>14.0</td>\n      <td>reviewed</td>\n      <td>pr</td>\n      <td>pr</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-02-23T06:50:49.137Z</td>\n      <td>38.487900</td>\n      <td>72.812200</td>\n      <td>10.000</td>\n      <td>4.50</td>\n      <td>mb</td>\n      <td>21.0</td>\n      <td>171.00</td>\n      <td>1.264000</td>\n      <td>0.5600</td>\n      <td>...</td>\n      <td>2023-02-23T07:28:35.040Z</td>\n      <td>106 km WNW of Murghob, Tajikistan</td>\n      <td>earthquake</td>\n      <td>8.00</td>\n      <td>2.000</td>\n      <td>0.140000</td>\n      <td>15.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-02-23T06:18:13.280Z</td>\n      <td>-8.548300</td>\n      <td>-77.625400</td>\n      <td>66.540</td>\n      <td>4.70</td>\n      <td>mb</td>\n      <td>44.0</td>\n      <td>154.00</td>\n      <td>3.503000</td>\n      <td>0.4700</td>\n      <td>...</td>\n      <td>2023-02-23T07:15:40.040Z</td>\n      <td>22 km SW of Quiches, Peru</td>\n      <td>earthquake</td>\n      <td>8.10</td>\n      <td>8.700</td>\n      <td>0.064000</td>\n      <td>74.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-02-23T03:36:09.429Z</td>\n      <td>-18.287000</td>\n      <td>-177.826100</td>\n      <td>524.983</td>\n      <td>4.90</td>\n      <td>mb</td>\n      <td>40.0</td>\n      <td>63.00</td>\n      <td>3.626000</td>\n      <td>0.6700</td>\n      <td>...</td>\n      <td>2023-02-23T03:50:54.040Z</td>\n      <td>NaN</td>\n      <td>earthquake</td>\n      <td>13.43</td>\n      <td>10.963</td>\n      <td>0.108000</td>\n      <td>27.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2023-02-23T03:18:57.663Z</td>\n      <td>-30.391600</td>\n      <td>-71.685600</td>\n      <td>34.097</td>\n      <td>4.40</td>\n      <td>ml</td>\n      <td>33.0</td>\n      <td>187.00</td>\n      <td>0.285000</td>\n      <td>0.9400</td>\n      <td>...</td>\n      <td>2023-02-23T03:30:52.786Z</td>\n      <td>52 km WNW of Ovalle, Chile</td>\n      <td>earthquake</td>\n      <td>5.75</td>\n      <td>5.597</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>guc</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2023-02-23T02:18:00.671Z</td>\n      <td>38.336200</td>\n      <td>73.106700</td>\n      <td>10.000</td>\n      <td>4.70</td>\n      <td>mb</td>\n      <td>52.0</td>\n      <td>60.00</td>\n      <td>1.529000</td>\n      <td>0.9400</td>\n      <td>...</td>\n      <td>2023-02-23T02:37:56.040Z</td>\n      <td>Tajikistan-Xinjiang border region</td>\n      <td>earthquake</td>\n      <td>6.56</td>\n      <td>1.897</td>\n      <td>0.059000</td>\n      <td>87.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2023-02-23T02:07:46.940Z</td>\n      <td>38.182200</td>\n      <td>73.234600</td>\n      <td>10.000</td>\n      <td>4.80</td>\n      <td>mb</td>\n      <td>21.0</td>\n      <td>180.00</td>\n      <td>1.712000</td>\n      <td>0.8100</td>\n      <td>...</td>\n      <td>2023-02-23T02:22:17.040Z</td>\n      <td>64 km W of Murghob, Tajikistan</td>\n      <td>earthquake</td>\n      <td>6.80</td>\n      <td>1.992</td>\n      <td>0.226000</td>\n      <td>6.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2023-02-23T02:03:42.690Z</td>\n      <td>18.079300</td>\n      <td>-68.088800</td>\n      <td>78.000</td>\n      <td>3.54</td>\n      <td>md</td>\n      <td>14.0</td>\n      <td>234.00</td>\n      <td>0.157700</td>\n      <td>0.2000</td>\n      <td>...</td>\n      <td>2023-02-23T02:42:39.960Z</td>\n      <td>64 km ESE of Boca de Yuma, Dominican Republic</td>\n      <td>earthquake</td>\n      <td>1.66</td>\n      <td>1.340</td>\n      <td>0.060000</td>\n      <td>6.0</td>\n      <td>reviewed</td>\n      <td>pr</td>\n      <td>pr</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2023-02-23T01:35:57.658Z</td>\n      <td>38.182400</td>\n      <td>73.279400</td>\n      <td>10.000</td>\n      <td>4.90</td>\n      <td>mb</td>\n      <td>27.0</td>\n      <td>166.00</td>\n      <td>1.735000</td>\n      <td>0.7000</td>\n      <td>...</td>\n      <td>2023-02-23T01:49:27.040Z</td>\n      <td>60 km W of Murghob, Tajikistan</td>\n      <td>earthquake</td>\n      <td>7.86</td>\n      <td>1.985</td>\n      <td>0.177000</td>\n      <td>10.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2023-02-23T01:33:25.965Z</td>\n      <td>59.261500</td>\n      <td>-153.135600</td>\n      <td>77.400</td>\n      <td>2.50</td>\n      <td>ml</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.5700</td>\n      <td>...</td>\n      <td>2023-02-23T01:35:49.711Z</td>\n      <td>69 km W of Nanwalek, Alaska</td>\n      <td>earthquake</td>\n      <td>NaN</td>\n      <td>0.700</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>automatic</td>\n      <td>ak</td>\n      <td>ak</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2023-02-23T01:06:05.337Z</td>\n      <td>38.180400</td>\n      <td>72.782900</td>\n      <td>10.000</td>\n      <td>4.60</td>\n      <td>mb</td>\n      <td>20.0</td>\n      <td>163.00</td>\n      <td>1.506000</td>\n      <td>0.6400</td>\n      <td>...</td>\n      <td>2023-02-23T01:27:51.040Z</td>\n      <td>103 km W of Murghob, Tajikistan</td>\n      <td>earthquake</td>\n      <td>7.16</td>\n      <td>1.772</td>\n      <td>0.207000</td>\n      <td>7.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2023-02-23T00:55:14.540Z</td>\n      <td>38.147400</td>\n      <td>72.982400</td>\n      <td>10.000</td>\n      <td>5.00</td>\n      <td>mb</td>\n      <td>84.0</td>\n      <td>80.00</td>\n      <td>1.618000</td>\n      <td>0.8400</td>\n      <td>...</td>\n      <td>2023-02-23T01:13:06.040Z</td>\n      <td>Tajikistan</td>\n      <td>earthquake</td>\n      <td>4.66</td>\n      <td>1.768</td>\n      <td>0.071000</td>\n      <td>63.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2023-02-23T00:37:40.397Z</td>\n      <td>38.072600</td>\n      <td>73.207700</td>\n      <td>20.522</td>\n      <td>6.80</td>\n      <td>mww</td>\n      <td>128.0</td>\n      <td>20.00</td>\n      <td>1.783000</td>\n      <td>0.9600</td>\n      <td>...</td>\n      <td>2023-02-23T05:49:51.952Z</td>\n      <td>67 km W of Murghob, Tajikistan</td>\n      <td>earthquake</td>\n      <td>6.93</td>\n      <td>4.439</td>\n      <td>0.065000</td>\n      <td>23.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2023-02-22T23:44:02.170Z</td>\n      <td>39.145168</td>\n      <td>-123.260002</td>\n      <td>1.870</td>\n      <td>2.91</td>\n      <td>md</td>\n      <td>26.0</td>\n      <td>68.00</td>\n      <td>0.068420</td>\n      <td>0.0800</td>\n      <td>...</td>\n      <td>2023-02-23T06:29:31.972Z</td>\n      <td>4km W of Ukiah, CA</td>\n      <td>earthquake</td>\n      <td>0.21</td>\n      <td>8.330</td>\n      <td>0.180000</td>\n      <td>32.0</td>\n      <td>automatic</td>\n      <td>nc</td>\n      <td>nc</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2023-02-22T23:14:55.147Z</td>\n      <td>64.931800</td>\n      <td>-147.252000</td>\n      <td>148.800</td>\n      <td>2.90</td>\n      <td>ml</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.2900</td>\n      <td>...</td>\n      <td>2023-02-22T23:18:24.121Z</td>\n      <td>12 km WNW of Two Rivers, Alaska</td>\n      <td>earthquake</td>\n      <td>NaN</td>\n      <td>14.300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>automatic</td>\n      <td>ak</td>\n      <td>ak</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2023-02-22T23:06:22.156Z</td>\n      <td>53.653800</td>\n      <td>98.458200</td>\n      <td>9.707</td>\n      <td>4.70</td>\n      <td>mb</td>\n      <td>64.0</td>\n      <td>68.00</td>\n      <td>3.724000</td>\n      <td>0.6700</td>\n      <td>...</td>\n      <td>2023-02-23T00:02:22.040Z</td>\n      <td>122 km WSW of Ikey, Russia</td>\n      <td>earthquake</td>\n      <td>7.42</td>\n      <td>4.501</td>\n      <td>0.053000</td>\n      <td>106.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2023-02-22T22:05:53.566Z</td>\n      <td>10.179100</td>\n      <td>125.063300</td>\n      <td>10.000</td>\n      <td>4.60</td>\n      <td>mb</td>\n      <td>44.0</td>\n      <td>120.00</td>\n      <td>3.131000</td>\n      <td>0.4600</td>\n      <td>...</td>\n      <td>2023-02-23T04:25:16.518Z</td>\n      <td>6 km WNW of Liloan, Philippines</td>\n      <td>earthquake</td>\n      <td>10.85</td>\n      <td>1.887</td>\n      <td>0.077000</td>\n      <td>51.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2023-02-22T21:39:43.663Z</td>\n      <td>59.184300</td>\n      <td>-151.517400</td>\n      <td>2.000</td>\n      <td>2.50</td>\n      <td>ml</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0800</td>\n      <td>...</td>\n      <td>2023-02-22T21:42:36.481Z</td>\n      <td>25 km SE of Port Graham, Alaska</td>\n      <td>earthquake</td>\n      <td>NaN</td>\n      <td>3.800</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>automatic</td>\n      <td>ak</td>\n      <td>ak</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2023-02-22T21:18:44.007Z</td>\n      <td>23.963800</td>\n      <td>121.774100</td>\n      <td>8.774</td>\n      <td>4.40</td>\n      <td>mb</td>\n      <td>34.0</td>\n      <td>83.00</td>\n      <td>0.266000</td>\n      <td>0.5100</td>\n      <td>...</td>\n      <td>2023-02-22T23:06:06.040Z</td>\n      <td>17 km E of Hualien City, Taiwan</td>\n      <td>earthquake</td>\n      <td>2.29</td>\n      <td>4.572</td>\n      <td>0.126000</td>\n      <td>18.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2023-02-22T20:01:57.120Z</td>\n      <td>38.820999</td>\n      <td>-122.853996</td>\n      <td>2.640</td>\n      <td>3.22</td>\n      <td>ml</td>\n      <td>52.0</td>\n      <td>49.00</td>\n      <td>0.006912</td>\n      <td>0.0500</td>\n      <td>...</td>\n      <td>2023-02-23T03:23:48.835Z</td>\n      <td>10km WNW of The Geysers, CA</td>\n      <td>earthquake</td>\n      <td>0.15</td>\n      <td>0.240</td>\n      <td>0.445000</td>\n      <td>10.0</td>\n      <td>automatic</td>\n      <td>nc</td>\n      <td>nc</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2023-02-22T18:22:12.682Z</td>\n      <td>38.157700</td>\n      <td>-117.889100</td>\n      <td>8.700</td>\n      <td>3.50</td>\n      <td>ml</td>\n      <td>13.0</td>\n      <td>80.53</td>\n      <td>0.059000</td>\n      <td>0.1197</td>\n      <td>...</td>\n      <td>2023-02-23T02:03:07.570Z</td>\n      <td>32 km SE of Mina, Nevada</td>\n      <td>earthquake</td>\n      <td>NaN</td>\n      <td>1.200</td>\n      <td>0.210000</td>\n      <td>9.0</td>\n      <td>reviewed</td>\n      <td>nn</td>\n      <td>nn</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2023-02-22T18:18:25.802Z</td>\n      <td>-35.700800</td>\n      <td>-73.161700</td>\n      <td>35.000</td>\n      <td>4.30</td>\n      <td>mb</td>\n      <td>39.0</td>\n      <td>150.00</td>\n      <td>1.045000</td>\n      <td>0.7200</td>\n      <td>...</td>\n      <td>2023-02-22T21:45:30.040Z</td>\n      <td>NaN</td>\n      <td>earthquake</td>\n      <td>4.88</td>\n      <td>1.955</td>\n      <td>0.238000</td>\n      <td>7.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2023-02-22T17:47:55.177Z</td>\n      <td>-35.748700</td>\n      <td>-73.151800</td>\n      <td>35.000</td>\n      <td>4.00</td>\n      <td>mb</td>\n      <td>26.0</td>\n      <td>198.00</td>\n      <td>0.998000</td>\n      <td>0.6600</td>\n      <td>...</td>\n      <td>2023-02-22T21:40:26.040Z</td>\n      <td>78 km WNW of Cauquenes, Chile</td>\n      <td>earthquake</td>\n      <td>4.98</td>\n      <td>2.007</td>\n      <td>0.519000</td>\n      <td>1.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2023-02-22T15:56:47.583Z</td>\n      <td>61.257000</td>\n      <td>-150.320300</td>\n      <td>250.000</td>\n      <td>2.50</td>\n      <td>ml</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.9300</td>\n      <td>...</td>\n      <td>2023-02-22T21:02:30.323Z</td>\n      <td>NaN</td>\n      <td>earthquake</td>\n      <td>NaN</td>\n      <td>5.100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>automatic</td>\n      <td>ak</td>\n      <td>ak</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2023-02-22T15:51:39.859Z</td>\n      <td>-30.038900</td>\n      <td>-72.203500</td>\n      <td>12.059</td>\n      <td>4.00</td>\n      <td>mwr</td>\n      <td>23.0</td>\n      <td>179.00</td>\n      <td>0.801000</td>\n      <td>1.1600</td>\n      <td>...</td>\n      <td>2023-02-22T17:15:55.040Z</td>\n      <td>83 km W of Coquimbo, Chile</td>\n      <td>earthquake</td>\n      <td>2.83</td>\n      <td>6.844</td>\n      <td>0.050000</td>\n      <td>38.0</td>\n      <td>reviewed</td>\n      <td>us</td>\n      <td>us</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2023-02-22T15:48:21.690Z</td>\n      <td>40.555832</td>\n      <td>-124.212669</td>\n      <td>14.430</td>\n      <td>3.21</td>\n      <td>ml</td>\n      <td>21.0</td>\n      <td>177.00</td>\n      <td>0.234100</td>\n      <td>0.1200</td>\n      <td>...</td>\n      <td>2023-02-23T00:44:05.211Z</td>\n      <td>5km ESE of Ferndale, CA</td>\n      <td>earthquake</td>\n      <td>0.78</td>\n      <td>0.740</td>\n      <td>0.302000</td>\n      <td>10.0</td>\n      <td>automatic</td>\n      <td>nc</td>\n      <td>nc</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2023-02-22T15:18:06.170Z</td>\n      <td>18.231000</td>\n      <td>-68.260500</td>\n      <td>96.000</td>\n      <td>3.76</td>\n      <td>md</td>\n      <td>15.0</td>\n      <td>205.00</td>\n      <td>0.307600</td>\n      <td>0.2700</td>\n      <td>...</td>\n      <td>2023-02-22T15:54:41.369Z</td>\n      <td>Mona Passage</td>\n      <td>earthquake</td>\n      <td>1.72</td>\n      <td>1.080</td>\n      <td>0.110000</td>\n      <td>11.0</td>\n      <td>reviewed</td>\n      <td>pr</td>\n      <td>pr</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2023-02-22T14:11:28.720Z</td>\n      <td>19.904167</td>\n      <td>-155.158333</td>\n      <td>43.590</td>\n      <td>3.24</td>\n      <td>ml</td>\n      <td>45.0</td>\n      <td>256.00</td>\n      <td>NaN</td>\n      <td>0.1200</td>\n      <td>...</td>\n      <td>2023-02-23T08:07:59.549Z</td>\n      <td>5 km NW of Honomu, Hawaii</td>\n      <td>earthquake</td>\n      <td>0.59</td>\n      <td>0.950</td>\n      <td>0.127770</td>\n      <td>25.0</td>\n      <td>reviewed</td>\n      <td>hv</td>\n      <td>hv</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2023-02-22T13:12:26.230Z</td>\n      <td>18.798667</td>\n      <td>-65.057500</td>\n      <td>17.130</td>\n      <td>3.09</td>\n      <td>md</td>\n      <td>8.0</td>\n      <td>259.00</td>\n      <td>0.454100</td>\n      <td>0.2100</td>\n      <td>...</td>\n      <td>2023-02-22T13:35:31.700Z</td>\n      <td>Puerto Rico region</td>\n      <td>earthquake</td>\n      <td>1.15</td>\n      <td>1.640</td>\n      <td>0.037425</td>\n      <td>4.0</td>\n      <td>reviewed</td>\n      <td>pr</td>\n      <td>pr</td>\n    </tr>\n  </tbody>\n</table>\n<p>30 rows Ã— 22 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "id": "e3d64e9c-11d9-435b-bee2-525662d846ae",
      "cell_type": "markdown",
      "source": "## Part 2: NumPy and Pandas\n## NumPy",
      "metadata": {}
    },
    {
      "id": "f67fd619-1559-4ffd-ae59-af94b6e18be7",
      "cell_type": "markdown",
      "source": "### Exercise 1: Creating arrays\n#### Steps:\n* Import numpy under the alias np\n* Create an array of 10 ones\n* Create an array of integers 1 to 20\n* Create a 5 x 5 matrix of ones with a dtype int.",
      "metadata": {}
    },
    {
      "id": "db1e5b99-4535-4c39-969f-75542bf5768e",
      "cell_type": "code",
      "source": "import numpy as np\n\narr1 = np.tile(1, 10)\nprint(arr1)\n\narr2 = np.arange(1, 21, 1, dtype=\"int\")\nprint(arr2)\n\narr3 = np.ones((5, 5), dtype=\"int\")\nprint(arr3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[1 1 1 1 1 1 1 1 1 1]\n[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n[[1 1 1 1 1]\n [1 1 1 1 1]\n [1 1 1 1 1]\n [1 1 1 1 1]\n [1 1 1 1 1]]\n"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "4b1b1201-09ea-4cfc-8a21-2b23e39892f3",
      "cell_type": "markdown",
      "source": "### Exercise 2: Create and reshape an array\n#### Steps:\n* Create an 3D matrix of 3 x 3 x 3 full of random numbers drawn from a standard normal distribution using `np.random.randn()`\n* Reshape the above array into shape (27,)",
      "metadata": {}
    },
    {
      "id": "f0bec9a0-c64b-4b1e-939f-ef682e345058",
      "cell_type": "code",
      "source": "arr1 = np.random.rand(3, 3, 3)\n\narr1a= arr1.reshape(27,)\nprint(arr1a)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[0.38732295 0.86964684 0.47217119 0.26663197 0.06566132 0.4687501\n 0.93009997 0.96642025 0.90483439 0.48938599 0.40672499 0.14459002\n 0.84715336 0.32018714 0.10008607 0.24050824 0.68377639 0.56703338\n 0.32072269 0.42664816 0.23461484 0.9285551  0.8160937  0.87834548\n 0.2574642  0.72186535 0.43362229]\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "2f233686-c280-4a65-98ad-8cc739d2123f",
      "cell_type": "markdown",
      "source": "### Exercise 3: Create a linearly spaced array\n#### Steps:\n* Create an array of 20 linearly spaced numbers between 1 and 10.",
      "metadata": {}
    },
    {
      "id": "3170bebb-4650-4381-93f6-91a81fa8a883",
      "cell_type": "code",
      "source": "arr1 = np.linspace(1, 10, 20)\nprint(arr1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[ 1.          1.47368421  1.94736842  2.42105263  2.89473684  3.36842105\n  3.84210526  4.31578947  4.78947368  5.26315789  5.73684211  6.21052632\n  6.68421053  7.15789474  7.63157895  8.10526316  8.57894737  9.05263158\n  9.52631579 10.        ]\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "2cab3eca-ea89-4b08-9b31-6f115d067aef",
      "cell_type": "markdown",
      "source": "### Exercise 4: Create and index an array\n#### Steps:\n* Run the following code to create an array of shape 4 x 4\n* Use indexing to produce the outputs shown below:\n```python\n20\n```\n```python\narray([[ 9, 10],\n       [14, 15],\n       [19, 20],\n       [24, 25]])\n```\n```python\narray([ 6,  7,  8,  9, 10])\n```",
      "metadata": {}
    },
    {
      "id": "640625cc-ab3a-4bce-9e12-bb34a0aea818",
      "cell_type": "code",
      "source": "import numpy as np\na = np.arange(1, 26).reshape(5, -1)\nprint(a[3, 4])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "20\n"
        }
      ],
      "execution_count": 6
    },
    {
      "id": "49a2b20c-640d-4459-88c8-cb606266bd9e",
      "cell_type": "code",
      "source": "a = np.arange(1, 26).reshape(5, -1)\nprint(a[1:, 3:5])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[ 9 10]\n [14 15]\n [19 20]\n [24 25]]\n"
        }
      ],
      "execution_count": 7
    },
    {
      "id": "dedcd227-261d-4eaf-b900-cd4c17fee0e1",
      "cell_type": "code",
      "source": "a = np.arange(1, 26).reshape(5, -1)\nprint(a[1, 0:5])",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[ 6  7  8  9 10]\n"
        }
      ],
      "execution_count": 8
    },
    {
      "id": "fd76ab2d-371e-445a-896a-b6248f36266d",
      "cell_type": "markdown",
      "source": "### Exercise 5: Calculate the sum of all numbers\n#### Steps:\n* Calculate the sum of all the numbers in `a`",
      "metadata": {}
    },
    {
      "id": "38c67dc6-a6fb-4598-a964-0db79e27c225",
      "cell_type": "code",
      "source": "a = np.arange(1, 26).reshape(5, -1)\nprint(np.sum(a))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "325\n"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "01a59d54-e895-4277-aa0b-273f181c234e",
      "cell_type": "markdown",
      "source": "### Exercise 6: Calculate the sum of each row\n#### Steps:\n* Calculate the sum of each row in `a`",
      "metadata": {}
    },
    {
      "id": "d34aa14c-fe7c-4cf3-bd12-738bb500bdae",
      "cell_type": "code",
      "source": "a = np.arange(1, 26).reshape(5, -1)\nprint(np.sum(a, axis=1))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[ 15  40  65  90 115]\n"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "548ae42e-1d69-4c1d-9caa-8073550f9f7e",
      "cell_type": "markdown",
      "source": "### Exercise 7: Boolean mask: extract values greater than the mean\n#### Steps:\n* Extract all values of `a` greater than the mean of `a` using a boolean mask",
      "metadata": {}
    },
    {
      "id": "6bdbdb5d-cc7c-4324-82f9-7c0feecf60d4",
      "cell_type": "code",
      "source": "a = np.arange(1, 26).reshape(5, -1)\nmean = np.mean(a)\nprint(mean)\n\nboolmask = a > mean\nprint(a[boolmask])",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "13.0\n[14 15 16 17 18 19 20 21 22 23 24 25]\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "7ffc6ccc-30d6-489a-b3bb-9c4ef9c0c82d",
      "cell_type": "markdown",
      "source": "## Pandas",
      "metadata": {}
    },
    {
      "id": "d0d0b413-2f72-4b98-a487-cb08cf30c896",
      "cell_type": "markdown",
      "source": "### Exercise 1: Importing Pandas\n#### Steps:\n* Import pandas with the alias pd",
      "metadata": {}
    },
    {
      "id": "671b8581-008f-4858-8856-739f8cae851b",
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "id": "6ad956c5-831d-494b-bb53-ca9ab9c231a0",
      "cell_type": "markdown",
      "source": "### Exercise 2: Import a CSV dataset\n#### Steps:\n* Import the dataset as a dataframe named `df` from this url: <https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv>",
      "metadata": {}
    },
    {
      "id": "53071291-de63-41c4-b929-51d372388ec0",
      "cell_type": "code",
      "source": "dataset = 'food_consumption (1).csv'\n\nfood_consumption_df = pd.read_csv('food_consumption (1).csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "033b699f-fdf2-45bc-bf50-15626a3fc0d6",
      "cell_type": "markdown",
      "source": "### Exercise 3: Rows and columns in the dataframe\n#### Steps:\n* Print the dataframe ",
      "metadata": {}
    },
    {
      "id": "26b9af79-c0b1-4780-bfe0-a77bc68acc14",
      "cell_type": "code",
      "source": "print(food_consumption_df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "         country             food_category  consumption  co2_emmission\n0      Argentina                      Pork        10.51          37.20\n1      Argentina                   Poultry        38.66          41.53\n2      Argentina                      Beef        55.48        1712.00\n3      Argentina               Lamb & Goat         1.56          54.63\n4      Argentina                      Fish         4.36           6.96\n...          ...                       ...          ...            ...\n1425  Bangladesh        Milk - inc. cheese        21.91          31.21\n1426  Bangladesh  Wheat and Wheat Products        17.47           3.33\n1427  Bangladesh                      Rice       171.73         219.76\n1428  Bangladesh                  Soybeans         0.61           0.27\n1429  Bangladesh   Nuts inc. Peanut Butter         0.72           1.27\n\n[1430 rows x 4 columns]\n"
        }
      ],
      "execution_count": 17
    },
    {
      "id": "daa153a4-e0f1-4690-a0e4-5449521d831d",
      "cell_type": "markdown",
      "source": "### Exercise 4: Mean of category in the whole dataset\n#### Steps:\n* Within the dataframe, select the Co2 Emission column and calcultate the mean of that column\n* Print the calculated mean value to the output",
      "metadata": {}
    },
    {
      "id": "b7e903b7-1346-46c7-ae63-1a9b786d49d2",
      "cell_type": "code",
      "source": "co2_mn = food_consumption_df[\"co2_emmission\"].mean()\nprint(co2_mn)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "74.383993006993\n"
        }
      ],
      "execution_count": 18
    },
    {
      "id": "98d76e0e-1c0f-4fe2-a8a1-c511163abf4b",
      "cell_type": "markdown",
      "source": "### Exercise 5: Max of category in the whole dataset\n#### Steps:\n* Within the dataframe, select the Co2 Emission column and calcultate the highest value in that column\n* Print the maximum Co2 emission value\n* Selects only the rows where the co2_emmission value equals the previously found maximum (co2_max)\n* Print the result to the output",
      "metadata": {}
    },
    {
      "id": "8fa8a67a-d6d0-418c-968d-c25fcd6e5519",
      "cell_type": "code",
      "source": "co2_max = food_consumption_df[\"co2_emmission\"].max()\nprint(co2_max)\n\nresult = food_consumption_df[food_consumption_df[\"co2_emmission\"] == co2_max]\nprint(result)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1712.0\n     country food_category  consumption  co2_emmission\n2  Argentina          Beef        55.48         1712.0\n"
        }
      ],
      "execution_count": 19
    },
    {
      "id": "95b469cf-75e6-4fa9-95ef-519a786b7147",
      "cell_type": "markdown",
      "source": "The maximum `co2_emmission` (1712.0) was Beef from Argentia ",
      "metadata": {}
    },
    {
      "id": "2af0a9b2-6668-490c-a8ba-3b65efd2d093",
      "cell_type": "markdown",
      "source": "### Exercise 6: Calculate countries produce more emissions than 1000 Kg/CO2\n#### Steps:\n* Filter the dataframe to keep only rows with co2_emmission values greater than 1000 Kg/CO2\n* Print filered results",
      "metadata": {}
    },
    {
      "id": "5e63f092-e530-4741-937c-89754a455e7d",
      "cell_type": "code",
      "source": "co2_py = food_consumption_df[food_consumption_df[\"co2_emmission\"] > 1000]\nprint(co2_py)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "       country food_category  consumption  co2_emmission\n2    Argentina          Beef        55.48        1712.00\n13   Australia          Beef        33.86        1044.85\n57         USA          Beef        36.24        1118.29\n90      Brazil          Beef        39.25        1211.17\n123    Bermuda          Beef        33.15        1022.94\n"
        }
      ],
      "execution_count": 20
    },
    {
      "id": "33b9d857-2be6-4ac4-a211-6de5db7dcd81",
      "cell_type": "markdown",
      "source": "\nThe results show that 5 countries produce more than 1000 Kg/CO2/person/year for Beef",
      "metadata": {}
    },
    {
      "id": "01020dd4-53af-462d-9b0d-38e214e2f9ca",
      "cell_type": "markdown",
      "source": "### Exercise 7: Calculate country consumes the least amount of beef\n#### Steps:\n* Filter the dataset to include only beef data\n* Finds the minimum consumption value\n* Print the country associated with that minimum consumption",
      "metadata": {}
    },
    {
      "id": "01444fe9-afcf-4d80-8a66-877b013f1262",
      "cell_type": "code",
      "source": "beef_df = food_consumption_df[food_consumption_df[\"food_category\"] == \"Beef\"][[\"country\", \"consumption\"]]\n\nbeef_min = beef_df[\"consumption\"].min()\n\ncountry_beef_min = beef_df[beef_df[\"consumption\"] == beef_min]\n\nprint(country_beef_min)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "      country  consumption\n1410  Liberia         0.78\n"
        }
      ],
      "execution_count": 21
    },
    {
      "id": "eb1693a0-6540-4cde-87df-f9edfa4d0daa",
      "cell_type": "markdown",
      "source": "### Exercise 8: Calculate total emission of meat products in dataset\n#### Steps:\n* Define meat categories\n* Filter the dataframe to include only those categories\n* Sum CO2 emissions for each category\n* Sum across all categories to get total emissions\n* Print the total",
      "metadata": {}
    },
    {
      "id": "553aa35e-3e6f-4d28-9813-908f89f05c9f",
      "cell_type": "code",
      "source": "meat_categories = [\"Pork\", \"Poultry\", \"Fish\", \"Lamb & Goat\", \"Beef\"]\n\nmeat_df = food_consumption_df[food_consumption_df[\"food_category\"].isin(meat_categories)]\n\nmeat_sum = meat_df.groupby(\"food_category\")[\"co2_emmission\"].sum()\n\ntotal_meat_emmiss = meat_sum.sum()\n\nprint(total_meat_emmiss)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "74441.13\n"
        }
      ],
      "execution_count": 22
    },
    {
      "id": "11eaa108-1512-458a-a8ae-adcd7db63fad",
      "cell_type": "markdown",
      "source": "### Exercise 9: Calculate total emission of all other products in dataset\n#### Steps:\n* Filter out meat categories to get only non-meat data\n* Sum CO2 emissions for each non-meat category\n* Sum across all non-meat categories to get the total\n* Print the total",
      "metadata": {}
    },
    {
      "id": "d2da67ed-353e-41ef-822f-3d1b60363884",
      "cell_type": "code",
      "source": "non_meat_df = food_consumption_df[~food_consumption_df[\"food_category\"].isin(meat_categories)]\n\nnon_meat_sum = non_meat_df.groupby(\"food_category\")[\"co2_emmission\"].sum()\n\ntotal_non_meat_emmiss = non_meat_sum.sum()\n\nprint(total_non_meat_emmiss)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "31927.98\n"
        }
      ],
      "execution_count": 23
    },
    {
      "id": "05121460-245f-48d5-9d6b-2afb241746f8",
      "cell_type": "markdown",
      "source": "## Final Exercise",
      "metadata": {}
    },
    {
      "id": "6699c718-87d8-445c-90fe-63e4fc7c8526",
      "cell_type": "markdown",
      "source": "### 1. Import pandas and read CSV dataset\n#### Steps:\n* Import pandas with the alias pd\n* Import and read the the world_cities.csv dataset as a dataframe named `df` ",
      "metadata": {}
    },
    {
      "id": "d9e7ba0f-1649-4830-8a0f-97328e6b64a7",
      "cell_type": "code",
      "source": "import pandas as pd\n\ndataset = 'world_cities.csv'\n\nworld_cities_df = pd.read_csv('world_cities.csv')\n\nprint(world_cities_df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "                     city       country     pop    lat    lon  capital\n0      'Abasan al-Jadidah     Palestine    5629  31.31  34.34        0\n1      'Abasan al-Kabirah     Palestine   18999  31.32  34.35        0\n2            'Abdul Hakim      Pakistan   47788  30.55  72.11        0\n3      'Abdullah-as-Salam        Kuwait   21817  29.36  47.98        0\n4                   'Abud     Palestine    2456  32.03  35.07        0\n...                   ...           ...     ...    ...    ...      ...\n43640           az-Zubayr          Iraq  124611  30.39  47.71        0\n43641            az-Zulfi  Saudi Arabia   54070  26.30  44.80        0\n43642       az-Zuwaytinah         Libya   21984  30.95  20.12        0\n43643        s-Gravenhage   Netherlands  479525  52.07   4.30        0\n43644     s-Hertogenbosch   Netherlands  135529  51.68   5.30        0\n\n[43645 rows x 6 columns]\n"
        }
      ],
      "execution_count": 24
    },
    {
      "id": "db9bc6aa-f3af-48ff-90c1-4e7eff387841",
      "cell_type": "markdown",
      "source": "### 2. Calcuate new column\n#### Steps:\n* Convert the pop column (population) to millions, divide it by 1,000,000\n* Save it in a new colum called \"pop_M\"\n* Print the first few rows to confirm that the new \"pop_M\" has appeared\n",
      "metadata": {}
    },
    {
      "id": "6918e6d5-5b7c-400b-a052-9c3f30251e8d",
      "cell_type": "code",
      "source": "world_cities_df['pop_M'] = world_cities_df['pop'] / 1_000_000\nprint(world_cities_df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "                 city    country    pop    lat    lon  capital     pop_M\n0  'Abasan al-Jadidah  Palestine   5629  31.31  34.34        0  0.005629\n1  'Abasan al-Kabirah  Palestine  18999  31.32  34.35        0  0.018999\n2        'Abdul Hakim   Pakistan  47788  30.55  72.11        0  0.047788\n3  'Abdullah-as-Salam     Kuwait  21817  29.36  47.98        0  0.021817\n4               'Abud  Palestine   2456  32.03  35.07        0  0.002456\n"
        }
      ],
      "execution_count": 25
    },
    {
      "id": "289b9a06-b87c-4758-bf38-38c1998ddf74",
      "cell_type": "markdown",
      "source": "### 3. Remove column\n#### Steps:\n* Remove orginal column named \"pop\" using \"drop(columns= )\"\n* Assign and print it back to the dataframe to make sure the change is applied",
      "metadata": {}
    },
    {
      "id": "1be88b1c-214c-438b-be8a-dfadf548bac0",
      "cell_type": "code",
      "source": "world_cities_df = world_cities_df.drop(columns=['pop'])\nprint(world_cities_df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "                 city    country    lat    lon  capital     pop_M\n0  'Abasan al-Jadidah  Palestine  31.31  34.34        0  0.005629\n1  'Abasan al-Kabirah  Palestine  31.32  34.35        0  0.018999\n2        'Abdul Hakim   Pakistan  30.55  72.11        0  0.047788\n3  'Abdullah-as-Salam     Kuwait  29.36  47.98        0  0.021817\n4               'Abud  Palestine  32.03  35.07        0  0.002456\n"
        }
      ],
      "execution_count": 26
    },
    {
      "id": "a5743538-7465-4950-8b40-f57dac6d0b08",
      "cell_type": "markdown",
      "source": "### 4. Subset a city starting with the letter F\n#### Steps:\n* Filter rows where city starts with \"F\", by accessing the \"F\" city column as strigs\n* Print the results",
      "metadata": {}
    },
    {
      "id": "f77b8996-9fc4-426a-a731-696205832cd4",
      "cell_type": "code",
      "source": "F_cities = world_cities_df[world_cities_df['city'].str.startswith('F')]\nprint(F_cities)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "             city           country    lat     lon  capital     pop_M\n11083       Fa'id             Egypt  30.32   32.31        0  0.019609\n11084        Faaa  French Polynesia -17.54 -149.59        0  0.029740\n11085      Faaaha  French Polynesia -16.60 -151.46        0  0.000451\n11086     Faaborg           Denmark  55.10   10.25        0  0.007235\n11087      Faaite  French Polynesia -16.75 -145.23        0  0.000366\n...           ...               ...    ...     ...      ...       ...\n11989      Fuzuli        Azerbaijan  39.60   48.15        0  0.026932\n11990  Fyodorovka        Kazakhstan  51.21   51.98        0  0.005355\n11991  Fyodorovka        Kazakhstan  53.64   62.69        0  0.008148\n11992    Fyresdal            Norway  59.18    8.10        0  0.000359\n11993        Fyti            Cyprus  34.93   32.55        0  0.000102\n\n[911 rows x 6 columns]\n"
        }
      ],
      "execution_count": 27
    },
    {
      "id": "f5b5a21a-c400-40c6-9448-02c264a9d0d6",
      "cell_type": "markdown",
      "source": "### 5. Subset the five biggest cities from the country where F city is\n#### Steps:\n* I have chosen the city of Faaa in French Polynesia\n* Filter cities for French Polynesia\n* Sort the \"pop_M\" in descending order\n* Select only the top five cities\n* Print results",
      "metadata": {}
    },
    {
      "id": "e033d4d0-a1e1-4930-b890-a18191263dca",
      "cell_type": "code",
      "source": "fpoly_cities = F_cities[F_cities['country'] == 'French Polynesia']\n\nfpoly_cities_sorted = fpoly_cities.sort_values(by='pop_M', ascending=False)\n\ntop_five_cities = fpoly_cities_sorted.head(5)\n\nprint(top_five_cities)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "         city           country    lat     lon  capital     pop_M\n11084    Faaa  French Polynesia -17.54 -149.59        0  0.029740\n11089  Faanui  French Polynesia -16.47 -151.74        0  0.002186\n11090  Faaone  French Polynesia -17.66 -149.29        0  0.001798\n11217    Fare  French Polynesia -16.69 -151.01        0  0.001575\n11499   Fitii  French Polynesia -16.72 -151.02        0  0.001167\n"
        }
      ],
      "execution_count": 28
    },
    {
      "id": "93a1dd23-1ad6-48ef-b8a1-29468019bb63",
      "cell_type": "markdown",
      "source": "The results show that the five biggest cities from French Polynesia where the city of Faaa is.",
      "metadata": {}
    }
  ]
}